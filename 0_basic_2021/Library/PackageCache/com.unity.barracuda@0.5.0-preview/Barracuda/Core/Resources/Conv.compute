#pragma kernel Conv2D
#pragma kernel Conv2D_RegisterBlock4x2
//#pragma kernel Conv2D_L1Cached64_RegisterBlock4x4
//#pragma kernel Conv2D_L1Cached32_RegisterBlock4x4
#pragma kernel Conv2DKernelKxK_T16x16_R4x4 BLOCK_SIZE=4                                             SUFFIX=KernelKxK_T16x16_R
#pragma kernel Conv2DKernelKxK_StrictC16K64_T16x16_R4x4 BLOCK_SIZE=4 STRICT_CHANNELS=1              SUFFIX=KernelKxK_StrictC16K64_T16x16_R
#pragma kernel Conv2DKernel1x1_StrictC16K64_T16x16_R4x4 BLOCK_SIZE=4 KERNEL_1x1=1 STRICT_CHANNELS=1 SUFFIX=Kernel1x1_StrictC16K64_T16x16_R

#pragma kernel DepthwiseConv2D

#pragma kernel Conv2DTrans

//Tested 2x2, 3x3 and 5x5 kernels with groupsize [8,8], [8,16], [16,16] and [16,32] (this one not in 5x5 as it does not fit in 32k)
//k=5x5 t=[16,16] fast consistently faster or equal to other configuration both on AMDVega and RTX2080 (tested with kernel size 2x2x32x32, input size 128x128x32)
//however this configuration is quite LDS bound performance profile might be very different on hardware without on chip LDS. This is especially true for smaller kernel
//as a lot of LDS will be reserved but not used, reducing the amount of cache used.
#pragma kernel Conv2DTrans_KernelCached_K5x5_T16x16 MAX_KERNEL_SIZE=5 GROUP_SIZE_X=16 GROUP_SIZE_Y=16

#pragma kernel KernelWinograd_3x3
#pragma kernel Conv2DWinograd_2x2_3x3

#include "Tensor.cginc"

TENSOR_DECL(X)
TENSOR_DECL(K)
TENSOR_DECL(B)
TENSOR_DECL(WBK)
TENSOR_DECL_RW(O)

uint4 _Pad;
uint4 _Stride;

#define DEBUG_CHECK_BOUNDS 0

// Conv2DBlock64x64_4x4 + index optimizations
//        T
//      -1|0             -1|0
// 16: 142|142ms        144|155ms

float ffma(float a, float b, float c) { return dot(float2(a,c), float2(b,1)); }
#define FUNC_NAME(KERNEL, SUFFIX, SIZE) KERNEL##SUFFIX##SIZE##x##SIZE
#define CACHE_NAME(KERNEL, SUFFIX, SIZE, TENSOR) KERNEL##SUFFIX##SIZE##x##SIZE##_Cache_##TENSOR

#define KERNEL_NAME Conv2D

#if BLOCK_SIZE == 4
#define CHANNELS_FIRST 0
#define BUF_OFFSET 0
#define CACHE_DEPTH 16 // This kernel code supports only CACHE_DEPTH=16, this value can not be changed
groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, X)[CACHE_DEPTH*16*BLOCK_SIZE+(1-CHANNELS_FIRST)*CACHE_DEPTH];
groupshared float CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, W)[CACHE_DEPTH*16*BLOCK_SIZE];
[numthreads(16,16,1)]
void FUNC_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE)(uint3 dispatchThreadID : SV_DispatchThreadID, uint3 groupThreadID : SV_GroupThreadID, uint threadIndex : SV_GroupIndex)
{
    DISPATCH_ARGS(K.kernelCount, O.width * O.height * O.batch, 1);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    // [W*H, Ky*Kx*In] * [Ky*Kx*In, Out] => [W*H, Out]

    #define X_ CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, X)
    #define W_ CACHE_NAME(KERNEL_NAME, SUFFIX, BLOCK_SIZE, W)

    int x = (int)dispatchThreadID.x * BLOCK_SIZE; // output_channels
    int y = (int)dispatchThreadID.y * BLOCK_SIZE; // batch*width*height
    int tx = (int)groupThreadID.x;
    int ty = (int)groupThreadID.y;
    int bx = ((int)dispatchThreadID.x - (int)groupThreadID.x) * BLOCK_SIZE;
    int by = ((int)dispatchThreadID.y - (int)groupThreadID.y) * BLOCK_SIZE;
    int ti = (int)threadIndex;
    uint w      = O.width;
    uint h      = O.height;
    int batches = X.batch;
    int channels = X.channels;
    int widthX  = X.width;
    int heightX = X.height;
    int strideX = X.channels;
    int strideK = K.channels;
    int strideO = O.channels;
    int offsetX = BUF_OFFSET;
    int offsetK = BUF_OFFSET;
    int offsetO = BUF_OFFSET;

    float4 dstA[4];
    dstA[0].x = B.Get(x+0); dstA[0].y = B.Get(x+1); dstA[0].z = B.Get(x+2); dstA[0].w = B.Get(x+3);
    dstA[1].x = B.Get(x+0); dstA[1].y = B.Get(x+1); dstA[1].z = B.Get(x+2); dstA[1].w = B.Get(x+3);
    dstA[2].x = B.Get(x+0); dstA[2].y = B.Get(x+1); dstA[2].z = B.Get(x+2); dstA[2].w = B.Get(x+3);
    dstA[3].x = B.Get(x+0); dstA[3].y = B.Get(x+1); dstA[3].z = B.Get(x+2); dstA[3].w = B.Get(x+3);

    int readK = strideK * (ti>>6) + bx + (ti&63) + offsetK;
    #if STRICT_CHANNELS
    #else
    bool maskK = (bx + (ti&63)) < strideK;
    #endif

#if CHANNELS_FIRST
    uint centroidId = by + (ti&63);
    #if KERNEL_1x1
    int readX = strideX * (ti>>6) + centroidId;
    #else
    int batch = centroidId / w / h;
    int topY = (centroidId / w % h) * _Stride.y - _Pad.y;
    int leftX = (centroidId % w) * _Stride.x - _Pad.x;
    int cornerId = batch * heightX * widthX + topY * widthX + leftX;
    int readX = strideX * (ti>>6) + cornerId;
    bool mask;
    #endif
#else
    uint4 centroidId = uint4(
        (by + (ti>>4) +  0),
        (by + (ti>>4) + 16),
        (by + (ti>>4) + 32),
        (by + (ti>>4) + 48));
    #if KERNEL_1x1
    int4 readX = strideX * centroidId + (ti&15);
    #else
    int4 batch = centroidId / w / h;
    int4 topY = (centroidId / w % h) * _Stride.y - _Pad.y;
    int4 leftX = (centroidId % w) * _Stride.x - _Pad.x;
    int4 cornerId = batch * heightX * widthX + topY * widthX + leftX;
    int4 readX = strideX * cornerId + (ti&15);
    bool4 mask;
    #endif
#endif

#if KERNEL_1x1
    {
        {
#else
    for (int dy = 0; dy < (int)K.GetKernelHeight(); dy++)
    {
        for (int dx = 0; dx < (int)K.GetKernelWidth(); dx++)
        {
            int kernelOffsetX = (dy * widthX + dx) * strideX;
            mask =
                batch < batches &&
                topY + dy >= 0 &&
                topY + dy < heightX &&
                leftX + dx >= 0 &&
                leftX + dx < widthX;

            // 256 threads (256=numthreads(16,16,1)=16*16*1) are communally loading
            // blocks of 64pixels x 16channels from the global memory
            //
            // One block is read from X and one from K tensor
            // 4 reads with 256 threads (4=64*16/256) are necessary for each block

#endif // KERNEL_1x1
            for (int i = 0; i < channels; i += CACHE_DEPTH)
            {
                #if STRICT_CHANNELS
                #else
                if (i + CACHE_DEPTH > channels)
                {
                    int channelRemainder = channels - i;
                    [unroll] for (int j = 0; j < 4; ++j)
                    {
                        bool maskChannelsK = ti < 64 * (channelRemainder - j * 4);
                        bool maskChannelsX =
                            #if CHANNELS_FIRST
                            maskChannelsK;
                            #else
                            (ti&15) < channelRemainder;
                            #endif

                        W_[((ti>>6)<<6) + ((ti&3)<<4) + ((ti&63)>>2) + 256*j] = K.MaskedGet(maskK & maskChannelsK, readK);
                        readK += strideK * max(0, min(channelRemainder - j * 4, 4));

                        #if CHANNELS_FIRST
                        X_[ti + 256*j] =
                            #if KERNEL_1x1
                            X.MaskedGet(maskChannelsX, readX + strideX * (i + j * 4) + offsetX);
                            #else
                            X.MaskedGet(mask && maskChannelsX, readX + strideX * (i + j * 4) + kernelOffsetX + offsetX);
                            #endif
                        #else
                        X_[(ti>>4) + 65*(ti&15) + 16*j] =
                            #if KERNEL_1x1
                            X.MaskedGet(maskChannelsX, readX[j] + i + offsetX);
                            #else
                            X.MaskedGet(mask[j] && maskChannelsX, readX[j] + i + kernelOffsetX + offsetX);
                            #endif
                        #endif
                    }
                }
                else
                #endif
                [unroll] for (int j = 0; j < 4; ++j)
                {
                    W_[((ti>>6)<<6) + ((ti&3)<<4) + ((ti&63)>>2) + 256*j] =
                        #if STRICT_CHANNELS
                        K.data[readK];
                        #else
                        K.MaskedGet(maskK, readK);
                        #endif
                    readK += strideK * 4;

                    #if CHANNELS_FIRST
                    X_[ti + 256*j] =
                        #if KERNEL_1x1
                        X.data[readX + strideX * (i + j * 4) + offsetX];
                        #else
                        X.MaskedGet(mask, readX + strideX * (i + j * 4) + kernelOffsetX + offsetX);
                        #endif
                    #else
                    X_[(ti>>4) + 65*(ti&15) + 16*j] =
                        #if KERNEL_1x1
                        X.data[readX[j] + i + offsetX];
                        #else
                        X.MaskedGet(mask[j], readX[j] + i + kernelOffsetX + offsetX);
                        #endif
                    #endif

                    #if DEBUG_CHECK_BOUNDS
                    if (
                        #if KERNEL_1x1
                        (readX[j] + i + offsetX < 0) ||
                        (readX[j] + i + offsetX >= (int)X.GetLength())
                        #else
                        (mask[j] && readX[j] + i + kernelOffsetX + offsetX < 0) ||
                        (mask[j] && readX[j] + i + kernelOffsetX + offsetX >= (int)X.GetLength())
                        #endif
                        )
                    {
                        // swamp X cache with dummy values when reading out of buffer
                        // this way we can detect out of buffer reads by comparing results from this kernel
                        // with the the reference implementation results
                        for (int q = 0; q < CACHE_DEPTH*16*BLOCK_SIZE+(1-CHANNELS_FIRST)*CACHE_DEPTH; ++q)
                            X_[q] = -1.0;
                    }
                    #endif
                }

                GroupMemoryBarrierWithGroupSync();

                int4 idX = int4(0,1,2,3);
                int4 idW = int4(0,16,32,48);
                int incX = 64 + (1-CHANNELS_FIRST);
                int incW = 64;

                for (int di = 0; di < CACHE_DEPTH; di++)
                {
                    float4 srcX = float4(
                        X_[idX.x + ty*4],
                        X_[idX.y + ty*4],
                        X_[idX.z + ty*4],
                        X_[idX.w + ty*4]);
                    float4 srcW = float4(
                        W_[idW.x + tx],
                        W_[idW.y + tx],
                        W_[idW.z + tx],
                        W_[idW.w + tx]
                    );
                    idX += incX;
                    idW += incW;

                    dstA[0].x = ffma(srcX.x, srcW.x, dstA[0].x);
                    dstA[0].y = ffma(srcX.x, srcW.y, dstA[0].y);
                    dstA[0].z = ffma(srcX.x, srcW.z, dstA[0].z);
                    dstA[0].w = ffma(srcX.x, srcW.w, dstA[0].w);

                    dstA[1].x = ffma(srcX.y, srcW.x, dstA[1].x);
                    dstA[1].y = ffma(srcX.y, srcW.y, dstA[1].y);
                    dstA[1].z = ffma(srcX.y, srcW.z, dstA[1].z);
                    dstA[1].w = ffma(srcX.y, srcW.w, dstA[1].w);

                    dstA[2].x = ffma(srcX.z, srcW.x, dstA[2].x);
                    dstA[2].y = ffma(srcX.z, srcW.y, dstA[2].y);
                    dstA[2].z = ffma(srcX.z, srcW.z, dstA[2].z);
                    dstA[2].w = ffma(srcX.z, srcW.w, dstA[2].w);

                    dstA[3].x = ffma(srcX.w, srcW.x, dstA[3].x);
                    dstA[3].y = ffma(srcX.w, srcW.y, dstA[3].y);
                    dstA[3].z = ffma(srcX.w, srcW.z, dstA[3].z);
                    dstA[3].w = ffma(srcX.w, srcW.w, dstA[3].w);
                }

                GroupMemoryBarrierWithGroupSync();
            }
        }
    }

    [unroll] for (int sy = 0; sy < 4 && y+sy < (int)w * (int)h * (int)O.batch; ++sy)
        [unroll] for (int sx = 0; sx < 4 && x+sx < strideO; ++sx)
            O.data[strideO * (y+sy) + x+sx + offsetO] = dstA[sy][sx];

    #undef X_
    #undef W_
}
#else
#endif
#undef CHANNELS_FIRST
#undef CACHE_DEPTH
#undef BUF_OFFSET
#undef KERNEL_NAME

NUMTHREADS((16,4,4), (8,4,4), (4,4,4))
void Conv2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;
    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.Get(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
        {
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos = uint2(x, y) * _Stride.xy + uint2(dx, dy);
                // @TODO: investigate
                // WARNING: had to move both y check into the loop (as opposed to checking y in parent loop) - due to potential bug in Metal compiler
                if (any(pos < leftCorner)) continue;
                if (any(pos >= rightCorner)) continue;

                for (uint c = 0; c < X.channels; ++c)
                    acc = fastfma(X.Get(n, pos.y - leftCorner.y, pos.x - leftCorner.x, c),  K.Get(dy, dx, c, k), acc);
            }
        }

        O.Set(n, y, x, k, acc);
    }
}


#define SIZE_W 4
#define SIZE_H 2
NUMTHREADS((64, 2, 2), (32, 2, 2), (16, 2, 2))
void Conv2D_RegisterBlock4x2(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x*SIZE_W >= O.width) return;
    if (y*SIZE_H >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;
    for (uint n = 0; n < O.batch; ++n)
    {
        float acc[SIZE_H*SIZE_W];
        uint q;
        [unroll]
        for (q = 0; q < SIZE_H*SIZE_W; ++q)
            acc[q] = B.Get(k);
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
        {
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
            {
                uint2 pos[SIZE_H*SIZE_W];
                [unroll]
                for (q = 0; q < SIZE_H*SIZE_W; ++q)
                    pos[q] = uint2(x*SIZE_W+(q%SIZE_W), y*SIZE_H+(q/SIZE_W)) * _Stride.xy + uint2(dx, dy);

                for (uint c = 0; c < X.channels; ++c)
                    [unroll]
                    for (q = 0; q < SIZE_H*SIZE_W; ++q)
                        if (all(pos[q] >= leftCorner) && all(pos[q] < rightCorner))
                            acc[q] = fastfma(X.Get(n, pos[q] - leftCorner, c), K.Get(dy, dx, c, k), acc[q]);
            }
        }

        [unroll]
        for (q = 0; q < SIZE_H*SIZE_W; ++q)
            O.Set(n, y*SIZE_H+(q/SIZE_W), x*SIZE_W+(q%SIZE_W), k, acc[q]);
    }
}
#undef SIZE_W
#undef SIZE_H

#define CONV2D_L1CACHED(L1CACHESIZE, SIZE, FMA) \
groupshared float Conv2D_L1Cached##L1CACHESIZE##_Reg_Loop_safe_X[SIZE*SIZE][L1CACHESIZE];\
[numthreads(L1CACHESIZE, 1, 1)]\
void Conv2D_L1Cached##L1CACHESIZE##_RegisterBlock##SIZE##x##SIZE(uint3 groupID : SV_GroupID, uint3 groupThreadID : SV_GroupThreadID)\
{\
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);\
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);\
\
    uint k = L1CACHESIZE * groupID.x + groupThreadID.x;\
    uint x = groupID.y;\
    uint y = groupID.z;\
\
    if (x*SIZE >= O.width) return;\
    if (y*SIZE >= O.height) return;\
\
    for (uint n = 0; n < O.batch; ++n)\
    {\
        float acc[SIZE*SIZE];\
        uint q;\
        [unroll]\
        for (q = 0; q < SIZE*SIZE; ++q)\
            acc[q] = B.SafeGet(k);\
\
        for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)\
        {\
            for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)\
            {\
                uint2 pos[SIZE*SIZE];\
                [unroll]\
                for (q = 0; q < SIZE*SIZE; ++q)\
                    pos[q] = uint2(x*SIZE+(q%SIZE), y*SIZE+(q/SIZE)) * _Stride.xy + uint2(dx, dy);\
\
                for (uint c = 0; c < X.channels; c += L1CACHESIZE)\
                {\
                    uint dc = groupThreadID.x;\
                    [unroll]\
                    for (q = 0; q < SIZE*SIZE; ++q)\
                        Conv2D_L1Cached##L1CACHESIZE##_Reg_Loop_safe_X[q][dc] = X.SafeGet(n, pos[q], c + dc, _Pad.xy);\
                    GroupMemoryBarrierWithGroupSync();\
\
                    if (k < K.channels)\
                    {\
                        uint kIndex = K.Index(dy, dx, c, k);\
                        for (dc = 0; dc < L1CACHESIZE; ++dc)\
                        {\
                            [unroll]\
                            for (q = 0; q < SIZE*SIZE; ++q)\
                                acc[q] = FMA(Conv2D_L1Cached##L1CACHESIZE##_Reg_Loop_safe_X[q][dc], K.data[kIndex], acc[q]);\
                            kIndex += K.channels;\
                        }\
                    }\
                    GroupMemoryBarrierWithGroupSync();\
                }\
            }\
        }\
\
        uint remainderW = (O.width - x*SIZE);\
        uint remainderH = (O.height - y*SIZE);\
\
        if (k < K.channels)\
            [unroll]\
            for (q = 0; q < SIZE*SIZE; ++q)\
                if (q/SIZE < remainderH && q%SIZE < remainderW)\
                    O.Set(n, y*SIZE+(q/SIZE), x*SIZE+(q%SIZE), k, acc[q]);\
    }\
\
}

CONV2D_L1CACHED(64,4, fastfma)
CONV2D_L1CACHED(32,4, fastfma)


// IDEA: iterate over channels in the inner loop - needs channels first layout
NUMTHREADS((16,4,4), (8,4,4), (4,4,4))
void DepthwiseConv2D(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 leftCorner = _Pad.xy;
    uint2 rightCorner = uint2(X.width, X.height) + _Pad.xy;

    uint2 leftKernelCorner = uint2(x, y) * _Stride.xy;
    uint2 rightKernelCorner = leftKernelCorner + uint2(K.GetKernelWidth(), K.GetKernelHeight());

    if (any(leftKernelCorner < leftCorner) || any(rightKernelCorner >= rightCorner))
    {
        // path with edge-cases checks
        for (uint n = 0; n < O.batch; ++n)
        {
            float acc = B.Get(k);
            for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
                for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
                {
                    uint2 pos = leftKernelCorner + uint2(dx, dy);
                    if (any(pos < leftCorner)) continue;
                    if (any(pos >= rightCorner)) continue;

                    acc = fastfma(
                        X.Get(n, pos.y - leftCorner.y, pos.x - leftCorner.x, k),
                        K.Get(dy, dx, 0, k),
                        acc);
                }

            O.Set(n, y, x, k, acc);
        }
    }
    else
    {
        // kernel is guaranteed to be within X,
        // no need to check against edge-cases
        leftKernelCorner -= leftCorner;
        for (uint n = 0; n < O.batch; ++n)
        {
            float acc = B.Get(k);
            for (uint dy = 0; dy < K.GetKernelHeight(); ++dy)
                for (uint dx = 0; dx < K.GetKernelWidth(); ++dx)
                {
                    uint2 pos = leftKernelCorner + uint2(dx, dy);

                    acc = fastfma(
                        X.Get(n, pos, k),
                        K.Get(dy, dx, 0, k),
                        acc);
                }

            O.Set(n, y, x, k, acc);
        }
    }
}

[numthreads(4,4,4)]
void Conv2DTrans(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    if (k >= K.channels) return;
    if (x >= O.width) return;
    if (y >= O.height) return;

    uint2 strideMask = _Stride.xy - 1;

    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = B.Get(k);
        for (uint dy = (y + _Pad.y) & strideMask.y; dy < K.GetKernelHeight(); dy += _Stride.y)
        {
            for (uint dx = (x + _Pad.x) & strideMask.x; dx < K.GetKernelWidth(); dx += _Stride.x)
            {

                uint2 pos = uint2(x + dx, y + dy);
                uint2 opos = (pos - _Pad.xy) / _Stride.xy;

                if (any(opos >= uint2(X.width, X.height))) continue;
                if (any(pos < _Pad.xy)) continue;

                for (uint c = 0; c < X.channels; ++c)
                {
                    acc = fastfma(  X.Get(n, opos.y, opos.x, c),
                                    K.Get(  K.GetKernelHeight() - 1 - dy,
                                            K.GetKernelWidth()  - 1 - dx, c, k),
                                    acc);
                }
            }
        }

        O.Set(n, y, x, k, acc);
    }
}

#if defined(MAX_KERNEL_SIZE) && defined(GROUP_SIZE_X) && defined(GROUP_SIZE_Y)
#define CONV2DTRANS_NAME(KERNEL,TGX,TGY) Conv2DTrans_KernelCached_K##KERNEL##x##KERNEL##_T##TGX##x##TGY
groupshared float Conv2DTrans_SharedKernel[MAX_KERNEL_SIZE][MAX_KERNEL_SIZE][GROUP_SIZE_X*GROUP_SIZE_Y];
groupshared float Conv2DTrans_SharedBias;
[numthreads(1,GROUP_SIZE_X,GROUP_SIZE_Y)]
void CONV2DTRANS_NAME(MAX_KERNEL_SIZE, GROUP_SIZE_X,GROUP_SIZE_Y)(uint3 dispatchThreadID : SV_DispatchThreadID, uint groupIndex: SV_GroupIndex)
{
    //Constraints:
    // C <= GROUP_SIZE_X*GROUP_SIZE_Y
    // K <= MAX_KERNEL_SIZExMAX_KERNEL_SIZE
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_SHARED2_ARGS4(X, K, B, WBK, O);

    uint k = dispatchThreadID.x;
    uint x = dispatchThreadID.y;
    uint y = dispatchThreadID.z;

    //Dispatch organisation:
    //  a thread  = write to [:,y,x,k] ie all batch but a single 2d pos and feature.
    //  a thread group = handle 1 feature in a GROUP_SIZExGROUP_SIZE x,y region, it loop other all batch, input channel count need to be <= GROUP_SIZE*GROUP_SIZE

    //LDS allocation
    //  we have 1 feature and up to GROUP_SIZE_X*GROUP_SIZE_Y channels per thread group, batch all use the same kernels,
    //  thus LDS is [MAX_KERNEL_SIZE][MAX_KERNEL_SIZE][GROUP_SIZE_X*GROUP_SIZE_Y]

    //Loading to LDS
    //  Each threads load a 2D kernel for a different channel into LDS
    for(uint dy = 0; dy < K.GetKernelWidth(); ++dy)
    {
        for(uint dx = 0; dx < K.GetKernelHeight(); ++dx)
        {
            uint channelToLoadIndex = groupIndex;
            if((channelToLoadIndex < X.channels) && (k < K.channels))
                Conv2DTrans_SharedKernel[dy][dx][channelToLoadIndex] = K.Get(K.GetKernelHeight() - 1 - dy,K.GetKernelWidth() - 1 - dx, channelToLoadIndex, k);
        }
    }
    //  first thread also load bias to LDS
    if (groupIndex == 0)
        Conv2DTrans_SharedBias = B.Get(k);

    //Wait for all load to complete
    GroupMemoryBarrierWithGroupSync();

    // Outside of target tensor, nothing to write to or compute exit.
    if (x >= O.width) return;
    if (y >= O.height) return;
    if (k >= K.channels) return;

    // Apply kernels from LDS to all batches and write result out (per batch as input differ)
    uint2 strideMask = _Stride.xy - 1;
    for (uint n = 0; n < O.batch; ++n)
    {
        float acc = Conv2DTrans_SharedBias;
        for (uint dy = (y + _Pad.y) & strideMask.y; dy < K.GetKernelHeight(); dy += _Stride.y)
        {
            for (uint dx = (x + _Pad.x) & strideMask.x; dx < K.GetKernelWidth(); dx += _Stride.x)
            {
                uint2 pos = uint2(x + dx, y + dy);
                uint2 opos = (pos - _Pad.xy) / _Stride.xy;
                if (any(opos >= uint2(X.width, X.height))) continue;
                if (any(pos < _Pad.xy)) continue;

                for (uint c = 0; c < X.channels; ++c)
                {
                    acc = fastfma(X.Get(n, opos.y, opos.x, c),
                        Conv2DTrans_SharedKernel[dy][dx][c],
                        acc);
                }
            }
        }
        O.Set(n, y, x, k, acc);
    }
}
#undef CONV2DTRANS_NAME
#endif //defined(MAX_KERNEL_SIZE) && defined(GROUP_SIZE_X) && defined(GROUP_SIZE_Y)




// https://github.com/andravin/wincnn
// https://arxiv.org/pdf/1509.09308.pdf
// Winograd: 4x4 image, 3x3 kernel, 2x2 output
static const float4x3 Winograd_G = float4x3(float3(1, 0, 0), float3(0.5, 0.5, 0.5), float3(0.5, -0.5, 0.5), float3(0, 0, 1));
static const float3x4 Winograd_GT = transpose(Winograd_G);
[numthreads(1, 1, 1)]
void KernelWinograd_3x3(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    TENSOR_ARGS2(X, O);

    uint k = dispatchThreadID.x;
    uint c = dispatchThreadID.y;


    // transform kernel -- N.B: do this offline offline
    float3x3 g;
    g[0][0] = X.Get(0, 0, c, k);
    g[0][1] = X.Get(0, 1, c, k);
    g[0][2] = X.Get(0, 2, c, k);
    g[1][0] = X.Get(1, 0, c, k);
    g[1][1] = X.Get(1, 1, c, k);
    g[1][2] = X.Get(1, 2, c, k);
    g[2][0] = X.Get(2, 0, c, k);
    g[2][1] = X.Get(2, 1, c, k);
    g[2][2] = X.Get(2, 2, c, k);

    float4x4 v = mul(Winograd_G, mul(g, Winograd_GT));

    O.Set(0, 0, c, k, v[0][0]);
    O.Set(1, 0, c, k, v[1][0]);
    O.Set(2, 0, c, k, v[2][0]);
    O.Set(3, 0, c, k, v[3][0]);
    O.Set(0, 1, c, k, v[0][1]);
    O.Set(1, 1, c, k, v[1][1]);
    O.Set(2, 1, c, k, v[2][1]);
    O.Set(3, 1, c, k, v[3][1]);
    O.Set(0, 2, c, k, v[0][2]);
    O.Set(1, 2, c, k, v[1][2]);
    O.Set(2, 2, c, k, v[2][2]);
    O.Set(3, 2, c, k, v[3][2]);
    O.Set(0, 3, c, k, v[0][3]);
    O.Set(1, 3, c, k, v[1][3]);
    O.Set(2, 3, c, k, v[2][3]);
    O.Set(3, 3, c, k, v[3][3]);
}

// BT x u x B, used mathematica to expresse the opperation using only +/-
float4x4 ApplyWinnogradB(float4x4 d)
{
    return float4x4(float4( d[0][0] - d[0][2] - d[2][0] + d[2][2],  d[0][1] + d[0][2] - d[2][1] - d[2][2], -d[0][1] + d[0][2] + d[2][1] - d[2][2], -d[0][1] + d[0][3] + d[2][1] - d[2][3]),
                    float4( d[1][0] - d[1][2] + d[2][0] - d[2][2],  d[1][1] + d[1][2] + d[2][1] + d[2][2], -d[1][1] + d[1][2] - d[2][1] + d[2][2], -d[1][1] + d[1][3] - d[2][1] + d[2][3]),
                    float4(-d[1][0] + d[1][2] + d[2][0] - d[2][2], -d[1][1] - d[1][2] + d[2][1] + d[2][2],  d[1][1] - d[1][2] - d[2][1] + d[2][2],  d[1][1] - d[1][3] - d[2][1] + d[2][3]),
                    float4(-d[1][0] + d[1][2] + d[3][0] - d[3][2], -d[1][1] - d[1][2] + d[3][1] + d[3][2],  d[1][1] - d[1][2] - d[3][1] + d[3][2],  d[1][1] - d[1][3] - d[3][1] + d[3][3])
        );

}
// A x u x A, used mathematica to expresse the opperation using only +/-
float2x2 ApplyWinnogradA(float4x4 uv)
{
    return float2x2(float2(uv[0][0] + uv[0][1] + uv[0][2] + uv[1][0] + uv[1][1] + uv[1][2] + uv[2][0] + uv[2][1] + uv[2][2], uv[0][1] - uv[0][2] + uv[0][3] + uv[1][1] - uv[1][2] + uv[1][3] + uv[2][1] - uv[2][2] + uv[2][3]),
                    float2(uv[1][0] + uv[1][1] + uv[1][2] - uv[2][0] - uv[2][1] - uv[2][2] + uv[3][0] + uv[3][1] + uv[3][2], uv[1][1] - uv[1][2] + uv[1][3] - uv[2][1] + uv[2][2] - uv[2][3] + uv[3][1] - uv[3][2] + uv[3][3])
        );
}

#undef GROUP_SIZE
#define GROUP_SIZE 2

// https://graphics.stanford.edu/~seander/bithacks.html#IntegerMinOrMax
#define min_no_branch(x,y) (y ^ ((x ^ y) & -(x < y)))

[numthreads(32, GROUP_SIZE, GROUP_SIZE)]
void Conv2DWinograd_2x2_3x3(uint3 dispatchThreadID : SV_DispatchThreadID)
{
    DISPATCH_ARGS(K.kernelCount, O.width, O.height);
    TENSOR_ARGS4(X, K, B, O);

    uint k = dispatchThreadID.x;
    if (k >= K.channels) return;

    uint2 index = 2 * dispatchThreadID.yz;

    uint2 pad = uint2(_Pad[0], _Pad[1]);
    uint2 XDim = uint2(X.width, X.height);


    // empirically faster on android
    // 0 if index - pad < 0 || index - pad > dim, given that we are dealing with uints, index - pad < 0 <=> index - pad ~= INTMAX
    // assuming usual input dim it should be fine.
    float4x4 paddingMask;
    paddingMask[0][0] = all(index.xy + uint2(0, 0) - pad < XDim);
    paddingMask[0][1] = all(index.xy + uint2(1, 0) - pad < XDim);
    paddingMask[0][2] = all(index.xy + uint2(2, 0) - pad < XDim);
    paddingMask[0][3] = all(index.xy + uint2(3, 0) - pad < XDim);
    paddingMask[1][0] = all(index.xy + uint2(0, 1) - pad < XDim);
    paddingMask[1][1] = all(index.xy + uint2(1, 1) - pad < XDim);
    paddingMask[1][2] = all(index.xy + uint2(2, 1) - pad < XDim);
    paddingMask[1][3] = all(index.xy + uint2(3, 1) - pad < XDim);
    paddingMask[2][0] = all(index.xy + uint2(0, 2) - pad < XDim);
    paddingMask[2][1] = all(index.xy + uint2(1, 2) - pad < XDim);
    paddingMask[2][2] = all(index.xy + uint2(2, 2) - pad < XDim);
    paddingMask[2][3] = all(index.xy + uint2(3, 2) - pad < XDim);
    paddingMask[3][0] = all(index.xy + uint2(0, 3) - pad < XDim);
    paddingMask[3][1] = all(index.xy + uint2(1, 3) - pad < XDim);
    paddingMask[3][2] = all(index.xy + uint2(2, 3) - pad < XDim);
    paddingMask[3][3] = all(index.xy + uint2(3, 3) - pad < XDim);


    for (uint n = 0; n < O.batch; ++n)
    {
        float2x2 acc = B.Get(k);

        for (uint c = 0; c < X.channels; ++c)
        {
            // 16 loads per thread
            float4x4 d;
            d[0][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 0) - pad, XDim-1), c);
            d[0][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 0) - pad, XDim-1), c);
            d[0][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 0) - pad, XDim-1), c);
            d[0][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 0) - pad, XDim-1), c);
            d[1][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 1) - pad, XDim-1), c);
            d[1][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 1) - pad, XDim-1), c);
            d[1][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 1) - pad, XDim-1), c);
            d[1][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 1) - pad, XDim-1), c);
            d[2][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 2) - pad, XDim-1), c);
            d[2][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 2) - pad, XDim-1), c);
            d[2][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 2) - pad, XDim-1), c);
            d[2][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 2) - pad, XDim-1), c);
            d[3][0] = X.Get(n, min_no_branch(index.xy + uint2(0, 3) - pad, XDim-1), c);
            d[3][1] = X.Get(n, min_no_branch(index.xy + uint2(1, 3) - pad, XDim-1), c);
            d[3][2] = X.Get(n, min_no_branch(index.xy + uint2(2, 3) - pad, XDim-1), c);
            d[3][3] = X.Get(n, min_no_branch(index.xy + uint2(3, 3) - pad, XDim-1), c);
            
            d = d * paddingMask;

            float4x4 v;
            v[0][0] = K.Get(0, 0, c, k);
            v[0][1] = K.Get(0, 1, c, k);
            v[0][2] = K.Get(0, 2, c, k);
            v[0][3] = K.Get(0, 3, c, k);
            v[1][0] = K.Get(1, 0, c, k);
            v[1][1] = K.Get(1, 1, c, k);
            v[1][2] = K.Get(1, 2, c, k);
            v[1][3] = K.Get(1, 3, c, k);
            v[2][0] = K.Get(2, 0, c, k);
            v[2][1] = K.Get(2, 1, c, k);
            v[2][2] = K.Get(2, 2, c, k);
            v[2][3] = K.Get(2, 3, c, k);
            v[3][0] = K.Get(3, 0, c, k);
            v[3][1] = K.Get(3, 1, c, k);
            v[3][2] = K.Get(3, 2, c, k);
            v[3][3] = K.Get(3, 3, c, k);

            float4x4 u = ApplyWinnogradB(d);
            float2x2 y = ApplyWinnogradA(v*u);
            acc += y;
        }

        // 4 writes per thread
        O.Set(n, index.y + 0, index.x + 0, k, acc[0][0]);
        O.Set(n, index.y + 1, index.x + 0, k, acc[1][0]);
        O.Set(n, index.y + 0, index.x + 1, k, acc[0][1]);
        O.Set(n, index.y + 1, index.x + 1, k, acc[1][1]);
    }
}
